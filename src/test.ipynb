{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ec2c8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nesymres.architectures.model import Model\n",
    "from nesymres.utils import load_metadata_hdf5\n",
    "from nesymres.dclasses import FitParams, NNEquation, BFGSParams\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sympy import lambdify\n",
    "import sympy\n",
    "import json\n",
    "import omegaconf\n",
    "from typing import Literal, Callable, Tuple, Union, Dict\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "device = \"cpu\" # NOTE: change to cuda if your GPU can handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "237d516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RES_DIR = \"../res/\"\n",
    "\n",
    "# load model config\n",
    "with open(os.path.join(RES_DIR, \"100m_eq_cfg.json\"), \"r\") as json_file:\n",
    "  eq_setting = json.load(json_file)\n",
    "\n",
    "cfg = omegaconf.OmegaConf.load(os.path.join(RES_DIR, \"100m_cfg.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b3293616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morris/miniconda3/envs/symreg/lib/python3.9/site-packages/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.3.3 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../res/100m.ckpt`\n"
     ]
    }
   ],
   "source": [
    "bfgs = BFGSParams(\n",
    "    activated= cfg.inference.bfgs.activated,\n",
    "    n_restarts=cfg.inference.bfgs.n_restarts,\n",
    "    add_coefficients_if_not_existing=cfg.inference.bfgs.add_coefficients_if_not_existing,\n",
    "    normalization_o=cfg.inference.bfgs.normalization_o,\n",
    "    idx_remove=cfg.inference.bfgs.idx_remove,\n",
    "    normalization_type=cfg.inference.bfgs.normalization_type,\n",
    "    stop_time=cfg.inference.bfgs.stop_time,\n",
    ")\n",
    "\n",
    "params_fit = FitParams(word2id=eq_setting[\"word2id\"], \n",
    "    id2word={int(k): v for k,v in eq_setting[\"id2word\"].items()}, \n",
    "    una_ops=eq_setting[\"una_ops\"], \n",
    "    bin_ops=eq_setting[\"bin_ops\"], \n",
    "    total_variables=list(eq_setting[\"total_variables\"]),  \n",
    "    total_coefficients=list(eq_setting[\"total_coefficients\"]),\n",
    "    rewrite_functions=list(eq_setting[\"rewrite_functions\"]),\n",
    "    bfgs=bfgs,\n",
    "    beam_size=cfg.inference.beam_size #This parameter is a tradeoff between accuracy and fitting time\n",
    ")\n",
    "\n",
    "# load model\n",
    "model = Model.load_from_checkpoint(os.path.join(RES_DIR, \"100m.ckpt\"), cfg=cfg.architecture).to(device)\n",
    "model.eval()\n",
    "\n",
    "fitfunc = partial(model.fitfunc, cfg_params=params_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "27d4e2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef test_hook(output, _hook: HookPoint):\\n    return torch.randn_like(output)\\nfor layer in range(4):\\n    register_decoder_hook(model, test_hook, HookPoint(layer, \"mlp\"))\\n'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class HookPoint:\n",
    "    layer: int\n",
    "    # Which part of the decoder layer to hook into. Can either be the MLP, a self-attention or cross-attention head.\n",
    "    component: Union[Literal[\"mlp\"], Tuple[Literal[\"self\", \"cross\"], int]]\n",
    "\n",
    "def register_decoder_hook(model: Model, hook_fn: Callable, hook: HookPoint) -> torch.utils.hooks.RemovableHandle:\n",
    "    \"\"\"\n",
    "    Hooks a function into the decoder part of the model. This allows for reading or manulipulating the output of a specific component.\n",
    "\n",
    "    NOTE: To remove the hook, the returned `RemovableHandle` must be called with `.remove()`.\n",
    "\n",
    "    # Args\n",
    "    * `model`: The model to hook into.\n",
    "    * `hook_fn`: Callable that takes the output of the hooked component as a`torch.Tensor` and the hooked location as a `HookPoint`.\n",
    "        The function should return an updated output.\n",
    "    * `hook`: Description of the component to hook into.\n",
    "    \"\"\"\n",
    "\n",
    "    def hook_wrapper(_module, _input, output):\n",
    "        if hook.component == \"mlp\":\n",
    "            output[0] = hook_fn(output[0], hook)\n",
    "        elif hook.component[0] == \"self\" or hook.component[0] == \"cross\":\n",
    "            head_idx = hook.component[1]\n",
    "\n",
    "            # multihead and self-attention layer have same number of heads\n",
    "            num_head = model.decoder_transfomer.layers[hook.layer].multihead_attn.num_heads\n",
    "            \n",
    "            # view data in terms of [seq_len x batch_size x num_head x head_dim])\n",
    "            seq_len, bsz, _ = output[0].size()\n",
    "            output_heads = output[0].view(seq_len, bsz, num_head, -1)\n",
    "\n",
    "            # hook output of specified head\n",
    "            output_heads[:, :, head_idx, :] = hook_fn(output_heads[:, :, head_idx, :], hook)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown hook component: {hook.component}\")\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    if hook.component == \"mlp\":\n",
    "        # hook into 2nd linear layer of MLP\n",
    "        return model.decoder_transfomer.layers[hook.layer].linear2.register_forward_hook(hook_wrapper)\n",
    "    elif hook.component[0] == \"self\":\n",
    "        # hook into self-attention layer\n",
    "        return model.decoder_transfomer.layers[hook.layer].self_attn.register_forward_hook(hook_wrapper)\n",
    "    elif hook.component[0] == \"cross\":\n",
    "        # hook into cross-attention layer\n",
    "        return model.decoder_transfomer.layers[hook.layer].multihead_attn.register_forward_hook(hook_wrapper)\n",
    "\n",
    "\n",
    "\n",
    "# If we uncomment the code below, we set all decoder MLP outputs to random values using interventions.\n",
    "#  As you'll see below, the model won't be able to fit the correct equation (:omg:).\n",
    "\"\"\"\n",
    "def test_hook(output, _hook: HookPoint):\n",
    "    return torch.randn_like(output)\n",
    "for layer in range(4):\n",
    "    register_decoder_hook(model, test_hook, HookPoint(layer, \"mlp\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b8041431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_equation(\n",
    "    eq: str, vars_used: list[str], vars_model: list[str],\n",
    "    point_count: int,\n",
    "    min_support: float, max_support: float,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Uniformly samples an equation using `sympy.lambdify`.\n",
    "\n",
    "    # Args\n",
    "    * `vars_used`: The names of the variables in the equation.\n",
    "    * `vars_model`: The total list of names of the variables the model expects. If the equation has less variables,\n",
    "        the input will be padded with zeros.\n",
    "    * `min_support` & `max_support`: The range of the domain to sample the equation from.\n",
    "\n",
    "    # Returns\n",
    "    A tuple containing:\n",
    "    * `X`: A tensor of shape [`point_count` x len(vars_model)] containing the sampled independent variables.\n",
    "    * `y`: A tensor of shape [`point_count`] containing the evaluated equation values.\n",
    "    \"\"\"\n",
    "\n",
    "    # of shape [N x D]\n",
    "    X = torch.zeros(point_count, len(vars_model))\n",
    "    X_dict = {}\n",
    "\n",
    "    # set used variables to random values\n",
    "    for idx, var in enumerate(vars_model):\n",
    "        if var in vars_used:\n",
    "            # sample uniformly and scale to the supported range\n",
    "            X[:, idx] = torch.rand(point_count) * (max_support - min_support) + min_support\n",
    "\n",
    "        X_dict[var] = X[:, idx]\n",
    "\n",
    "    # evaluate equation\n",
    "    y = sympy.lambdify(\",\".join(vars_model), eq)(**X_dict)\n",
    "\n",
    "    return X, y\n",
    "    \n",
    "def sample_equation_from_config(\n",
    "    eq: str, vars_used: list[str], point_count: int,\n",
    "    model_cfg: omegaconf.DictConfig,\n",
    "    eq_cfg: Dict,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Wrapper around `sample_equation` that uses the model and equation configuration to sample an equation.\n",
    "    \"\"\"\n",
    "\n",
    "    return sample_equation(\n",
    "        eq=eq,\n",
    "        vars_used=vars_used,\n",
    "        vars_model=eq_cfg[\"total_variables\"],\n",
    "        point_count=point_count,\n",
    "        min_support=model_cfg.dataset_train.fun_support[\"min\"],\n",
    "        max_support=model_cfg.dataset_train.fun_support[\"max\"],\n",
    "    )\n",
    "\n",
    "#X, y = sample_equation_from_config(\"exp(cos(x_1))\", [\"x_1\"], 500, cfg, eq_setting)\n",
    "X, y = sample_equation_from_config(\"sin(abs(x_1))+cos(abs(sin(x_2)))\", [\"x_1\", \"x_2\"], 500, cfg, eq_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a99b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('x_2+x_2', 'x_2-x_2'),\n",
       " ('x_2+cos(x_1)', 'x_2-cos(x_1)'),\n",
       " ('abs(x_2)+x_1', 'abs(x_2)-x_1'),\n",
       " ('x_3+cos(exp(sin(x_3)))', 'x_3-cos(exp(sin(x_3)))'),\n",
       " ('sin(sin(exp(x_2)))+x_1', 'sin(sin(exp(x_2)))-x_1'),\n",
       " ('sin(x_3)+abs(x_1)', 'sin(x_3)-abs(x_1)'),\n",
       " ('x_3+exp(x_3)', 'x_3-exp(x_3)'),\n",
       " ('x_3+x_3', 'x_3-x_3'),\n",
       " ('x_2+x_1', 'x_2-x_1'),\n",
       " ('x_1+x_1', 'x_1-x_1')]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_function(vars: list[str], identity_prob=0.5, nest_prob=0.3, max_depth=3) -> Tuple[str, list[str]]:\n",
    "    \"\"\" \n",
    "    Generates a potentially nested function assuming a uniform function distribution over operators.\n",
    "    This could, for example, be a combination of powers and trigonometric functions,\n",
    "    but no addition, substraction, multiplication or division.\n",
    "\n",
    "    # Args\n",
    "    * `identity_prob`: The probability that the identity function is used (i.e., a naked variable).\n",
    "    * `nest_prob`: In case the function is not an identity function, the probability of nesting a function.\n",
    "    * `max_depth`: Maximum nesting depth.\n",
    "\n",
    "    # Returns\n",
    "    A tuple containing:\n",
    "    * `equation`: The equation.\n",
    "    * `vars_used`: The independent variables in the function.\n",
    "    \"\"\"\n",
    "\n",
    "    # NOTE: functions that have incomplete domains are ignored (log, tan, etc.)\n",
    "    funcs = [\"abs\", \"cos\", \"exp\", \"sin\"]\n",
    "    \n",
    "    var = np.random.choice(vars)\n",
    "\n",
    "    if np.random.sample() < identity_prob:\n",
    "        # return naked variable\n",
    "        return var, [var]\n",
    "    else:\n",
    "        func = np.random.choice(funcs) + \"(\"\n",
    "\n",
    "        # nest function\n",
    "        depth = 1\n",
    "        while np.random.sample() < nest_prob and depth < max_depth:\n",
    "            func += np.random.choice(funcs) + \"(\"\n",
    "            depth += 1\n",
    "\n",
    "        # add variable\n",
    "        func += var\n",
    "        func += \")\" * depth\n",
    "\n",
    "        return func, [var]\n",
    "\n",
    "\n",
    "def generate_dataset_pairs(\n",
    "    strategy: Literal[\"sign-bias\", \"complexity-bias\"], point_count: int, num_eq: int,\n",
    "    model_cfg: omegaconf.DictConfig, eq_cfg: Dict,\n",
    "    second_dataset_sample_rate: int=None    \n",
    ") -> Tuple[torch.Tensor, torch.Tensor, list[str]]:\n",
    "    \"\"\"\n",
    "    Generates dataset pairs based on a given strategy. Depending on the strategy, random equations pairs\n",
    "    are generated, which are then sampled to form dataset pairs.\n",
    "    \n",
    "    # Args\n",
    "    * `strategy`:\n",
    "        1. If equal to `\"sign-bias\"`, equations are of the following form: `function_1(variable_1) Â± function_2(variable_2)`.\n",
    "        The first equation in the pair will apply the `+` operator and the second the `-` operator.\n",
    "        2. If equal to `\"complexity-bias\"`, equations are sampled randomly. Each dataset in a pair samples the same\n",
    "        equation, but the second dataset with a different sample rate determined by `second_dataset_sample_rate`.\n",
    "    * `point_count`: The number of points per dataset.\n",
    "    * `num_eq`: The number of equation pairs to generate.\n",
    "\n",
    "    # Returns\n",
    "    A tuple containing:\n",
    "    * `X`: A tensor of sampled equation input variables of size [2 x Ne x Np x D], where 2 denotes each pair,\n",
    "        Ne the number of equations, Np the number of points, and D the dimensionality.\n",
    "    * `y`: A tensor of shape [2 x Ne x Np] containing the evaluated equation values.\n",
    "    * `equations`: A list of size [Ne x 2] containing the generated equations.\n",
    "    \"\"\"\n",
    "    X = torch.empty((2, num_eq, point_count, len(eq_cfg[\"total_variables\"])))\n",
    "    y = torch.empty((2, num_eq, point_count))\n",
    "    equations = []\n",
    "\n",
    "    if strategy == \"sign-bias\":\n",
    "        for i in range(num_eq):\n",
    "            # generate equation\n",
    "            eq_part0, vars0 = generate_function(eq_cfg[\"total_variables\"])\n",
    "            eq_part1, vars1 = generate_function(eq_cfg[\"total_variables\"])\n",
    "            vars_used = vars0 + vars1\n",
    "\n",
    "            eq_plus = eq_part0 + \"+\" + eq_part1\n",
    "            eq_min = eq_part0 + \"-\" + eq_part1\n",
    "\n",
    "            # sample equation\n",
    "            Xe0, ye0 = sample_equation_from_config(eq_plus, vars_used, point_count, model_cfg, eq_cfg)\n",
    "            Xe1, ye1 = sample_equation_from_config(eq_min, vars_used, point_count, model_cfg, eq_cfg)\n",
    "\n",
    "            # store data\n",
    "            X[0, i], y[0, i] = Xe0, ye0\n",
    "            X[1, i], y[1, i] = Xe1, ye1\n",
    "            equations.append((eq_plus, eq_min))\n",
    "\n",
    "    elif strategy == \"complexity-bias\":\n",
    "        assert second_dataset_sample_rate != None, f\"second_dataset_sample_rate not set, but should be for strategy: {strategy}\"\n",
    "\n",
    "        for i in range(num_eq):\n",
    "            # generate equation\n",
    "            eq, vars_used = ... # TODO: generate equation\n",
    "\n",
    "            # sample equation\n",
    "            Xe0, ye0 = sample_equation_from_config(eq, vars_used, point_count, model_cfg, eq_cfg)\n",
    "            Xe1, ye1 = Xe0[::second_dataset_sample_rate], ye0[::second_dataset_sample_rate]\n",
    "\n",
    "            # store data\n",
    "            X[0, i], y[0, i] = Xe0, ye0\n",
    "            X[1, i], y[1, i] = Xe1, ye1\n",
    "            equations.append((eq, eq))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "    \n",
    "    return X, y, equations\n",
    "\n",
    "\n",
    "generate_dataset_pairs(\"sign-bias\", 10, 10, cfg, eq_setting)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e67847d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_predict(\n",
    "        model: Model, params: FitParams,\n",
    "        X: torch.Tensor = None, y: torch.Tensor = None, sequence: torch.Tensor = None, enc_embed: torch.Tensor = None,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Greedily predicts the next token in the sequence. Can be used in two ways:\n",
    "    1. If `X` and `y` are provided, the model will use them to compute the encoder embedding and then predict the next token.\n",
    "    2. If `enc_embed` is provided, the model will use this embedding directly to predict the next token.\n",
    "\n",
    "    # Args\n",
    "    * `X`: Function domain of shape [B, N, D], where B is the batch size, N is the number of samples,\n",
    "        and D the input dimensionality (no more than 3).\n",
    "    * `y`: Function image corresponding to `X` of shape [B, N].\n",
    "    * `sequence`: The initial tokens to predict the next token from of shape [B, S], where S is the maximum sequence length.\n",
    "        All samples in the batch are expected to be at the same current sequence length.\n",
    "    * `enc_embed`: Encoder embedding. Can be reused if the same `X` and `y` are used repeatedly for prediction.\n",
    "\n",
    "    # Returns\n",
    "    A tuple containing:\n",
    "    * `next_token`: The predicted token IDs for each sample in the batch.\n",
    "    * `sequence`: The sequence of tokens generated so far, which is updated with the predicted token.\n",
    "    * `enc_embed`: The encoder embedding, which may be resued.\n",
    "    \"\"\"\n",
    "\n",
    "    if enc_embed is None:\n",
    "        # compute encoder embedding\n",
    "        enc_input = torch.cat((X, y.unsqueeze(-1)), dim=-1).to(model.device)\n",
    "        enc_embed = model.enc(enc_input)\n",
    "\n",
    "    batch_size = enc_embed.size(0)\n",
    "\n",
    "    if sequence is None:\n",
    "        # initialize sequence with start token\n",
    "        sequence = torch.zeros((batch_size, model.cfg.length_eq), dtype=torch.long, device=model.device)\n",
    "        sequence[:, 0] = params.word2id[\"S\"]\n",
    "\n",
    "    cur_len = (sequence != 0).sum(dim=1).max().item()\n",
    "\n",
    "    # generate decoder masks\n",
    "    mask1, mask2 = model.make_trg_mask(\n",
    "        sequence[:, :cur_len]\n",
    "    )\n",
    "\n",
    "    # compute positional embeddings\n",
    "    pos = model.pos_embedding(\n",
    "        torch.arange(0, cur_len)\n",
    "            .unsqueeze(0)\n",
    "            .repeat(sequence.shape[0], 1)\n",
    "            .type_as(sequence)\n",
    "    )\n",
    "\n",
    "    # embed tokens\n",
    "    seq_embed = model.tok_embedding(sequence[:, :cur_len])\n",
    "    seq_embed = model.dropout(seq_embed + pos)\n",
    "\n",
    "    # run decoder\n",
    "    output = model.decoder_transfomer(\n",
    "        seq_embed.permute(1, 0, 2),\n",
    "        enc_embed.permute(1, 0, 2),\n",
    "        mask2.float(),\n",
    "        tgt_key_padding_mask=mask1.bool(),\n",
    "    )\n",
    "    output = model.fc_out(output)\n",
    "    output = output.permute(1, 0, 2).contiguous()\n",
    "\n",
    "    # add next token\n",
    "    # NOTE: softmax not really necessary here, but may come in handy later\n",
    "    token_probs = F.softmax(output[:, -1:, :], dim=-1).squeeze(1)\n",
    "    next_token = torch.argmax(token_probs, dim=-1)\n",
    "    sequence[:, cur_len] = next_token\n",
    "\n",
    "    return next_token, sequence, enc_embed\n",
    "\n",
    "def tokens_to_text(tokens: torch.Tensor, params: FitParams) -> list[str]:\n",
    "    \"\"\"\n",
    "    Converts a batches of token IDs to their corresponding text representations.\n",
    "\n",
    "    # Args\n",
    "    * `tokens`: Of shape [B, S], where B is the batch size and S is the maximum sequence length.\n",
    "    \"\"\"\n",
    "\n",
    "    decoded = []\n",
    "    for batch in tokens:\n",
    "        text = []\n",
    "\n",
    "        for token in batch:\n",
    "            if token.item() == params.word2id[\"F\"] or token.item() == 0:\n",
    "                break\n",
    "            text.append(params.id2word[token.item()])\n",
    "        decoded.append(\" \".join(text[1:])) # skip start token\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6766d9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morris/miniconda3/envs/symreg/lib/python3.9/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['add cos cos x_1 cos cos x_1']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial token prediction, this initializes the sequence and caches the encoder embedding (saves computation time).\n",
    "tok, seq, enc_embed = greedy_predict(model, params_fit, X.unsqueeze(0), y.unsqueeze(0))\n",
    "\n",
    "# repeatedly predict next token greedily\n",
    "for i in range(30):\n",
    "    seq = greedy_predict(model, params_fit, enc_embed=enc_embed, sequence=seq)[1]\n",
    "\n",
    "# this should result in (roughly) the correct equation\n",
    "tokens_to_text(seq, params_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "033a0348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morris/Documents/uni/msc/expl/NeuralSymbolicRegressionThatScales/src/nesymres/architectures/model.py:136: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X,device=self.device).unsqueeze(0)\n",
      "/home/morris/Documents/uni/msc/expl/NeuralSymbolicRegressionThatScales/src/nesymres/architectures/model.py:140: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y,device=self.device).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint of the encoder: 4.096e-05GB \n",
      "\n",
      "Constructing BFGS loss...\n",
      "Flag idx remove ON, Removing indeces with high values...\n",
      "checking input values range...\n",
      "Loss constructed, starting new BFGS optmization...\n",
      "Constructing BFGS loss...\n",
      "Flag idx remove ON, Removing indeces with high values...\n",
      "checking input values range...\n",
      "Loss constructed, starting new BFGS optmization...\n"
     ]
    }
   ],
   "source": [
    "# fit model with beam search instead of greedy + constant fitting (takes a lot longer)\n",
    "output = fitfunc(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5b30a42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all_bfgs_preds': ['((cos(cos(x_1)))+(cos(cos(x_1))))',\n",
       "  '((cos((cos(x_1))**(2)))+(cos(x_2)))'],\n",
       " 'all_bfgs_loss': [0.85989344, 1.0511613],\n",
       " 'best_bfgs_preds': ['((cos(cos(x_1)))+(cos(cos(x_1))))'],\n",
       " 'best_bfgs_loss': [0.85989344]}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here you can see the fitted equations\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symreg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
