{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ec2c8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import load_model, greedy_predict, tokens_to_text\n",
    "from hooks import HookPoint, register_decoder_hook\n",
    "from data import generate_dataset_pairs\n",
    "import torch\n",
    "import sympy\n",
    "\n",
    "device = \"cpu\" # NOTE: change to cuda if your GPU can handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b3293616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morris/miniconda3/envs/symreg/lib/python3.9/site-packages/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.3.3 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../res/100m.ckpt`\n"
     ]
    }
   ],
   "source": [
    "model = load_model(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "27d4e2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef test_hook(output, _hook: HookPoint):\\n    return torch.randn_like(output)\\n\\nfor layer in range(4):\\n    register_decoder_hook(model.model, test_hook, HookPoint(layer, \"mlp\"))\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we uncomment the code below, we set all decoder MLP outputs to random values using interventions.\n",
    "#  As you'll see below, the model won't be able to fit the correct equation (:omg:).\n",
    "\n",
    "\"\"\"\n",
    "def test_hook(output, _hook: HookPoint):\n",
    "    return torch.randn_like(output)\n",
    "\n",
    "for layer in range(4):\n",
    "    register_decoder_hook(model.model, test_hook, HookPoint(layer, \"mlp\"))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c4a99b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth function\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x_{1} x_{2} x_{3} + x_{1} x_{2} + x_{1}$"
      ],
      "text/plain": [
       "x_1*x_2*x_3 + x_1*x_2 + x_1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complexity_dataset = generate_dataset_pairs(\"complexity-bias\", 500, 10, model.model_cfg, model.eq_cfg, second_dataset_sample_rate=2)\n",
    "\n",
    "X = complexity_dataset[\"X0\"][0]\n",
    "y = complexity_dataset[\"y0\"][0]\n",
    "\n",
    "print(\"Ground truth function\")\n",
    "sympy.sympify(complexity_dataset[\"equations\"][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6766d9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morris/miniconda3/envs/symreg/lib/python3.9/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy predicted equation:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x_{1} \\left(x_{2} x_{3} + x_{2}\\right) + x_{1}$"
      ],
      "text/plain": [
       "x_1*(x_2*x_3 + x_2) + x_1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initial token prediction, this initializes the sequence and caches the encoder embedding (saves computation time).\n",
    "tok, seq, enc_embed = greedy_predict(model.model, model.params_fit, X.unsqueeze(0), y.unsqueeze(0))\n",
    "\n",
    "# repeatedly predict next token greedily\n",
    "for i in range(30):\n",
    "    seq = greedy_predict(model.model, model.params_fit, enc_embed=enc_embed, sequence=seq)[1]\n",
    "\n",
    "# this should result in (roughly) the correct equation\n",
    "greedy_pred = tokens_to_text(seq, model.params_fit)\n",
    "\n",
    "print(\"Greedy predicted equation:\")\n",
    "for eq in greedy_pred:\n",
    "    display(sympy.sympify(eq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "033a0348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint of the encoder: 4.096e-05GB \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morris/Documents/uni/msc/expl/NeuralSymbolicRegressionThatScales/src/nesymres/architectures/model.py:136: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X,device=self.device).unsqueeze(0)\n",
      "/home/morris/Documents/uni/msc/expl/NeuralSymbolicRegressionThatScales/src/nesymres/architectures/model.py:140: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y,device=self.device).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing BFGS loss...\n",
      "Flag idx remove ON, Removing indeces with high values...\n",
      "checking input values range...\n",
      "Attention, input values are very large. Optimization may fail due to numerical issues\n",
      "Loss constructed, starting new BFGS optmization...\n",
      "Constructing BFGS loss...\n",
      "Flag idx remove ON, Removing indeces with high values...\n",
      "checking input values range...\n",
      "Attention, input values are very large. Optimization may fail due to numerical issues\n",
      "Loss constructed, starting new BFGS optmization...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'all_bfgs_preds': ['((x_1)+((x_1)*((x_2)+((x_2)*(x_3)))))',\n",
       "  '((x_1)+((x_2)*((x_1)+((x_1)*(x_3)))))'],\n",
       " 'all_bfgs_loss': [1.7166933e-10, 1.0823885e-10],\n",
       " 'best_bfgs_preds': ['((x_1)+((x_2)*((x_1)+((x_1)*(x_3)))))'],\n",
       " 'best_bfgs_loss': [1.0823885e-10]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model with beam search instead of greedy + constant fitting (takes a lot longer)\n",
    "output = model.fitfunc(X, y) \n",
    "# here you can see the fitted equations\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6f90e4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best BFGS prediction:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x_{1} + x_{2} \\left(x_{1} x_{3} + x_{1}\\right)$"
      ],
      "text/plain": [
       "x_1 + x_2*(x_1*x_3 + x_1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best BFGS prediction:\")\n",
    "sympy.sympify(output[\"best_bfgs_preds\"][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symreg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
