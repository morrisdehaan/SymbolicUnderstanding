{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ec2c8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nesymres.architectures.model import Model\n",
    "from nesymres.utils import load_metadata_hdf5\n",
    "from nesymres.dclasses import FitParams, NNEquation, BFGSParams\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sympy import lambdify\n",
    "import sympy\n",
    "import json\n",
    "import omegaconf\n",
    "from typing import Literal, Callable, Tuple, Union, Dict\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "device = \"cpu\" # NOTE: change to cuda if your GPU can handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "237d516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RES_DIR = \"../res/\"\n",
    "\n",
    "# load model config\n",
    "with open(os.path.join(RES_DIR, \"100m_eq_cfg.json\"), \"r\") as json_file:\n",
    "  eq_setting = json.load(json_file)\n",
    "\n",
    "cfg = omegaconf.OmegaConf.load(os.path.join(RES_DIR, \"100m_cfg.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b3293616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morris/miniconda3/envs/symreg/lib/python3.9/site-packages/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.3.3 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../res/100m.ckpt`\n"
     ]
    }
   ],
   "source": [
    "bfgs = BFGSParams(\n",
    "    activated= cfg.inference.bfgs.activated,\n",
    "    n_restarts=cfg.inference.bfgs.n_restarts,\n",
    "    add_coefficients_if_not_existing=cfg.inference.bfgs.add_coefficients_if_not_existing,\n",
    "    normalization_o=cfg.inference.bfgs.normalization_o,\n",
    "    idx_remove=cfg.inference.bfgs.idx_remove,\n",
    "    normalization_type=cfg.inference.bfgs.normalization_type,\n",
    "    stop_time=cfg.inference.bfgs.stop_time,\n",
    ")\n",
    "\n",
    "params_fit = FitParams(word2id=eq_setting[\"word2id\"], \n",
    "    id2word={int(k): v for k,v in eq_setting[\"id2word\"].items()}, \n",
    "    una_ops=eq_setting[\"una_ops\"], \n",
    "    bin_ops=eq_setting[\"bin_ops\"], \n",
    "    total_variables=list(eq_setting[\"total_variables\"]),  \n",
    "    total_coefficients=list(eq_setting[\"total_coefficients\"]),\n",
    "    rewrite_functions=list(eq_setting[\"rewrite_functions\"]),\n",
    "    bfgs=bfgs,\n",
    "    beam_size=cfg.inference.beam_size #This parameter is a tradeoff between accuracy and fitting time\n",
    ")\n",
    "\n",
    "# load model\n",
    "model = Model.load_from_checkpoint(os.path.join(RES_DIR, \"100m.ckpt\"), cfg=cfg.architecture).to(device)\n",
    "model.eval()\n",
    "\n",
    "fitfunc = partial(model.fitfunc, cfg_params=params_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "27d4e2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef test_hook(output, _hook: HookPoint):\\n    return torch.randn_like(output)\\nfor layer in range(4):\\n    register_decoder_hook(model, test_hook, HookPoint(layer, \"mlp\"))\\n'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class HookPoint:\n",
    "    layer: int\n",
    "    # Which part of the decoder layer to hook into. Can either be the MLP, a self-attention or cross-attention head.\n",
    "    component: Union[Literal[\"mlp\"], Tuple[Literal[\"self\", \"cross\"], int]]\n",
    "\n",
    "def register_decoder_hook(model: Model, hook_fn: Callable, hook: HookPoint) -> torch.utils.hooks.RemovableHandle:\n",
    "    \"\"\"\n",
    "    Hooks a function into the decoder part of the model. This allows for reading or manulipulating the output of a specific component.\n",
    "\n",
    "    NOTE: To remove the hook, the returned `RemovableHandle` must be called with `.remove()`.\n",
    "\n",
    "    # Args\n",
    "    * `model`: The model to hook into.\n",
    "    * `hook_fn`: Callable that takes the output of the hooked component as a`torch.Tensor` and the hooked location as a `HookPoint`.\n",
    "        The function should return an updated output.\n",
    "    * `hook`: Description of the component to hook into.\n",
    "    \"\"\"\n",
    "\n",
    "    def hook_wrapper(_module, _input, output):\n",
    "        if hook.component == \"mlp\":\n",
    "            output[0] = hook_fn(output[0], hook)\n",
    "        elif hook.component[0] == \"self\" or hook.component[0] == \"cross\":\n",
    "            head_idx = hook.component[1]\n",
    "\n",
    "            # multihead and self-attention layer have same number of heads\n",
    "            num_head = model.decoder_transfomer.layers[hook.layer].multihead_attn.num_heads\n",
    "            \n",
    "            # view data in terms of [seq_len x batch_size x num_head x head_dim])\n",
    "            seq_len, bsz, _ = output[0].size()\n",
    "            output_heads = output[0].view(seq_len, bsz, num_head, -1)\n",
    "\n",
    "            # hook output of specified head\n",
    "            output_heads[:, :, head_idx, :] = hook_fn(output_heads[:, :, head_idx, :], hook)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown hook component: {hook.component}\")\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    if hook.component == \"mlp\":\n",
    "        # hook into 2nd linear layer of MLP\n",
    "        return model.decoder_transfomer.layers[hook.layer].linear2.register_forward_hook(hook_wrapper)\n",
    "    elif hook.component[0] == \"self\":\n",
    "        # hook into self-attention layer\n",
    "        return model.decoder_transfomer.layers[hook.layer].self_attn.register_forward_hook(hook_wrapper)\n",
    "    elif hook.component[0] == \"cross\":\n",
    "        # hook into cross-attention layer\n",
    "        return model.decoder_transfomer.layers[hook.layer].multihead_attn.register_forward_hook(hook_wrapper)\n",
    "\n",
    "\n",
    "\n",
    "# If we uncomment the code below, we set all decoder MLP outputs to random values using interventions.\n",
    "#  As you'll see below, the model won't be able to fit the correct equation (:omg:).\n",
    "\"\"\"\n",
    "def test_hook(output, _hook: HookPoint):\n",
    "    return torch.randn_like(output)\n",
    "for layer in range(4):\n",
    "    register_decoder_hook(model, test_hook, HookPoint(layer, \"mlp\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8041431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_equation(\n",
    "    eq: str, vars_used: list[str], vars_model: list[str],\n",
    "    point_count: int,\n",
    "    min_support: float, max_support: float,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Uniformly samples an equation using `sympy.lambdify`.\n",
    "\n",
    "    # Args\n",
    "    * `vars_used`: The names of the variables in the equation.\n",
    "    * `vars_model`: The total list of names of the variables the model expects. If the equation has less variables,\n",
    "        the input will be padded with zeros.\n",
    "    * `min_support` & `max_support`: The range of the domain to sample the equation from.\n",
    "\n",
    "    # Returns\n",
    "    A tuple containing:\n",
    "    * `torch.Tensor`: A tensor of shape [`point_count` x len(vars_model)] containing the sampled independent variables.\n",
    "    * `torch.Tensor`: A tensor of shape [`point_count`] containing the evaluated equation values.\n",
    "    \"\"\"\n",
    "\n",
    "    # of shape [N x D]\n",
    "    X = torch.zeros(point_count, len(vars_model))\n",
    "    X_dict = {}\n",
    "\n",
    "    # set used variables to random values\n",
    "    for idx, var in enumerate(vars_model):\n",
    "        if var in vars_used:\n",
    "            # sample uniformly and scale to the supported range\n",
    "            X[:, idx] = torch.rand(point_count) * (max_support - min_support) + min_support\n",
    "\n",
    "        X_dict[var] = X[:, idx]\n",
    "\n",
    "    # evaluate equation\n",
    "    y = sympy.lambdify(\",\".join(vars_model), eq)(**X_dict)\n",
    "\n",
    "    return X, y\n",
    "    \n",
    "def sample_equation_from_config(\n",
    "    eq: str, vars_used: list[str], point_count: int,\n",
    "    model_cfg: omegaconf.DictConfig,\n",
    "    eq_cfg: Dict,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Wrapper around `sample_equation` that uses the model and equation configuration to sample an equation.\n",
    "    \"\"\"\n",
    "\n",
    "    return sample_equation(\n",
    "        eq=eq,\n",
    "        vars_used=vars_used,\n",
    "        vars_model=eq_cfg[\"total_variables\"],\n",
    "        point_count=point_count,\n",
    "        min_support=model_cfg.dataset_train.fun_support[\"min\"],\n",
    "        max_support=model_cfg.dataset_train.fun_support[\"max\"],\n",
    "    )\n",
    "\n",
    "X, y = sample_equation_from_config(\"exp(cos(x_1))\", [\"x_1\"], 500, cfg, eq_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e67847d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_predict(\n",
    "        model: Model, params: FitParams,\n",
    "        X: torch.Tensor = None, y: torch.Tensor = None, sequence: torch.Tensor = None, enc_embed: torch.Tensor = None,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Greedily predicts the next token in the sequence. Can be used in two ways:\n",
    "    1. If `X` and `y` are provided, the model will use them to compute the encoder embedding and then predict the next token.\n",
    "    2. If `enc_embed` is provided, the model will use this embedding directly to predict the next token.\n",
    "\n",
    "    # Args\n",
    "    * `X`: Function domain of shape [B, N, D], where B is the batch size, N is the number of samples,\n",
    "        and D the input dimensionality (no more than 3).\n",
    "    * `y`: Function image corresponding to `X` of shape [B, N].\n",
    "    * `sequence`: The initial tokens to predict the next token from of shape [B, S], where S is the maximum sequence length.\n",
    "        All samples in the batch are expected to be at the same current sequence length.\n",
    "    * `enc_embed`: Encoder embedding. Can be reused if the same `X` and `y` are used repeatedly for prediction.\n",
    "\n",
    "    # Returns\n",
    "    A tuple containing:\n",
    "    * `torch.Tensor`: The predicted token IDs for each sample in the batch.\n",
    "    * `torch.Tensor`: The sequence of tokens generated so far, which is updated with the predicted token.\n",
    "    * `torch.Tensor`: The encoder embedding, which may be resued.\n",
    "    \"\"\"\n",
    "\n",
    "    if enc_embed is None:\n",
    "        # compute encoder embedding\n",
    "        enc_input = torch.cat((X, y.unsqueeze(-1)), dim=-1).to(model.device)\n",
    "        enc_embed = model.enc(enc_input)\n",
    "\n",
    "    batch_size = enc_embed.size(0)\n",
    "\n",
    "    if sequence is None:\n",
    "        # initialize sequence with start token\n",
    "        sequence = torch.zeros((batch_size, model.cfg.length_eq), dtype=torch.long, device=model.device)\n",
    "        sequence[:, 0] = params.word2id[\"S\"]\n",
    "\n",
    "    cur_len = (sequence != 0).sum(dim=1).max().item()\n",
    "\n",
    "    # generate decoder masks\n",
    "    mask1, mask2 = model.make_trg_mask(\n",
    "        sequence[:, :cur_len]\n",
    "    )\n",
    "\n",
    "    # compute positional embeddings\n",
    "    pos = model.pos_embedding(\n",
    "        torch.arange(0, cur_len)\n",
    "            .unsqueeze(0)\n",
    "            .repeat(sequence.shape[0], 1)\n",
    "            .type_as(sequence)\n",
    "    )\n",
    "\n",
    "    # embed tokens\n",
    "    seq_embed = model.tok_embedding(sequence[:, :cur_len])\n",
    "    seq_embed = model.dropout(seq_embed + pos)\n",
    "\n",
    "    # run decoder\n",
    "    output = model.decoder_transfomer(\n",
    "        seq_embed.permute(1, 0, 2),\n",
    "        enc_embed.permute(1, 0, 2),\n",
    "        mask2.float(),\n",
    "        tgt_key_padding_mask=mask1.bool(),\n",
    "    )\n",
    "    output = model.fc_out(output)\n",
    "    output = output.permute(1, 0, 2).contiguous()\n",
    "\n",
    "    # add next token\n",
    "    # NOTE: softmax not really necessary here, but may come in handy later\n",
    "    token_probs = F.softmax(output[:, -1:, :], dim=-1).squeeze(1)\n",
    "    next_token = torch.argmax(token_probs, dim=-1)\n",
    "    sequence[:, cur_len] = next_token\n",
    "\n",
    "    return next_token, sequence, enc_embed\n",
    "\n",
    "def tokens_to_text(tokens: torch.Tensor, params: FitParams) -> list[str]:\n",
    "    \"\"\"\n",
    "    Converts a batches of token IDs to their corresponding text representations.\n",
    "\n",
    "    # Args\n",
    "    * `tokens`: Of shape [B, S], where B is the batch size and S is the maximum sequence length.\n",
    "    \"\"\"\n",
    "\n",
    "    decoded = []\n",
    "    for batch in tokens:\n",
    "        text = []\n",
    "\n",
    "        for token in batch:\n",
    "            if token.item() == params.word2id[\"F\"] or token.item() == 0:\n",
    "                break\n",
    "            text.append(params.id2word[token.item()])\n",
    "        decoded.append(\" \".join(text[1:])) # skip start token\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6766d9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morris/miniconda3/envs/symreg/lib/python3.9/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['exp cos x_1']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial token prediction, this initializes the sequence and caches the encoder embedding (saves computation time).\n",
    "tok, seq, enc_embed = greedy_predict(model, params_fit, X.unsqueeze(0), y.unsqueeze(0))\n",
    "\n",
    "# repeatedly predict next token greedily\n",
    "for i in range(30):\n",
    "    seq = greedy_predict(model, params_fit, enc_embed=enc_embed, sequence=seq)[1]\n",
    "\n",
    "# this should result in (roughly) the correct equation\n",
    "tokens_to_text(seq, params_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "033a0348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morris/Documents/uni/msc/expl/NeuralSymbolicRegressionThatScales/src/nesymres/architectures/model.py:136: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X,device=self.device).unsqueeze(0)\n",
      "/home/morris/Documents/uni/msc/expl/NeuralSymbolicRegressionThatScales/src/nesymres/architectures/model.py:140: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y,device=self.device).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint of the encoder: 4.096e-05GB \n",
      "\n",
      "Constructing BFGS loss...\n",
      "Flag idx remove ON, Removing indeces with high values...\n",
      "checking input values range...\n",
      "Loss constructed, starting new BFGS optmization...\n",
      "Constructing BFGS loss...\n",
      "Flag idx remove ON, Removing indeces with high values...\n",
      "checking input values range...\n",
      "Loss constructed, starting new BFGS optmization...\n"
     ]
    }
   ],
   "source": [
    "# fit model with beam search instead of greedy + constant fitting (takes a lot longer)\n",
    "output = fitfunc(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5b30a42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all_bfgs_preds': ['(exp(cos(x_1)))', 'exp(1.00000001329477*cos(x_1))'],\n",
       " 'all_bfgs_loss': [0.0, 0.0],\n",
       " 'best_bfgs_preds': ['(exp(cos(x_1)))'],\n",
       " 'best_bfgs_loss': [0.0]}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here you can see the fitted equations\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symreg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
